/*
 * System detection and adaptive resource allocation functions
 * These functions are included in the main nextflow.config
 */

// System capability detection functions
def get_available_cpus() {
    try {
        def runtime = Runtime.getRuntime()
        return runtime.availableProcessors()
    } catch (Exception e) {
        return 4  // Fallback
    }
}

def get_available_memory() {
    try {
        def runtime = Runtime.getRuntime()
        def max_memory = runtime.maxMemory()
        return (max_memory / (1024 * 1024 * 1024)) as nextflow.util.MemoryUnit
    } catch (Exception e) {
        return 16.GB  // Fallback
    }
}

def detect_gpu_count() {
    try {
        def proc = "nvidia-smi --list-gpus".execute()
        proc.waitFor()
        if (proc.exitValue() == 0) {
            def output = proc.text
            return output.split('\n').findAll { it.trim() }.size()
        }
    } catch (Exception e) {
        // GPU detection failed
    }
    return 0
}

def get_gpu_memory_per_device() {
    try {
        def proc = "nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits".execute()
        proc.waitFor()
        if (proc.exitValue() == 0) {
            def output = proc.text.trim()
            def memories = output.split('\n').collect { it.trim() as Integer }
            return memories.min() * 1024 * 1024  // Convert MB to bytes
        }
    } catch (Exception e) {
        // GPU memory detection failed
    }
    return 0
}

def get_scale_factor() {
    // Scale resources based on dataset size hints
    def input_size = params.input_size ?: 'medium'
    switch (input_size) {
        case 'small':  return 0.5
        case 'medium': return 1.0
        case 'large':  return 1.5
        case 'xlarge': return 2.0
        default:       return 1.0
    }
}

def get_memory_scale_factor(process_type) {
    def base_factor = get_scale_factor()

    // Process-specific memory scaling
    def process_factors = [
        'validation': 0.5,
        'preprocess': 1.0,
        'single': 0.5,
        'low': 0.8,
        'medium': 1.0,
        'high': 1.5,
        'high_memory': 2.0,
        'gpu': 1.5,
        'analyze': 1.0,
        'visualize': 0.8
    ]

    def process_factor = process_factors[process_type] ?: 1.0
    return base_factor * process_factor
}

def get_system_type() {
    def available_memory = get_available_memory().toGiga()
    def available_cpus = get_available_cpus()
    def gpu_count = detect_gpu_count()

    if (gpu_count >= 4 && available_memory >= 256 && available_cpus >= 32) {
        return 'hpc_gpu'
    } else if (gpu_count >= 1 && available_memory >= 64 && available_cpus >= 16) {
        return 'workstation_gpu'
    } else if (available_memory >= 32 && available_cpus >= 8) {
        return 'workstation'
    } else {
        return 'laptop'
    }
}

// Adaptive resource functions for use in base.config
def adaptive_cpus(base_cpus) {
    def available = get_available_cpus()
    def scale_factor = get_scale_factor()
    def scaled = Math.ceil(base_cpus * scale_factor)
    return Math.min(scaled, available)
}

def adaptive_memory(base_memory, process_type) {
    def available = get_available_memory()
    def scale_factor = get_memory_scale_factor(process_type)
    def scaled = base_memory.toBytes() * scale_factor
    return Math.min(scaled, available * 0.8) as nextflow.util.MemoryUnit
}

def adaptive_gpu_memory(high_memory = false) {
    def base = high_memory ? 64.GB : 32.GB
    def gpu_count = detect_gpu_count()
    def gpu_memory_per_device = get_gpu_memory_per_device()

    if (gpu_count > 0 && gpu_memory_per_device > 0) {
        // Scale memory based on GPU memory availability
        def total_gpu_memory = gpu_count * gpu_memory_per_device
        def suggested_memory = Math.min(base.toBytes(), total_gpu_memory * 0.8)
        return suggested_memory as nextflow.util.MemoryUnit
    }

    return adaptive_memory(base, 'gpu')
}

def get_gpu_container_options() {
    def options = ''
    if (workflow.containerEngine == 'singularity') {
        options += ' --nv'
    } else if (workflow.containerEngine == 'docker') {
        options += ' --gpus all'
    }
    return options
}
