/*
 * -------------------------------------------------
 *  nf-core/resolvinf Nextflow base config file
 * -------------------------------------------------
 * Dynamic resource allocation based on system capabilities
 * and dataset size with fallback configurations
 */

// Import system detection functions
includeConfig 'system_detection.config'

process {
    // Dynamic defaults based on system capabilities
    cpus   = { check_max( Math.min(1, get_available_cpus()), 'cpus' ) }
    memory = { check_max( Math.min(6.GB, get_available_memory() * 0.1), 'memory' ) }
    time   = { check_max( 4.h * task.attempt, 'time' ) }

    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 1
    maxErrors     = '-1'

    // Adaptive resource labels
    withLabel:process_single {
        cpus   = { check_max( 1, 'cpus' ) }
        memory = { check_max( adaptive_memory(4.GB, 'single'), 'memory' ) }
        time   = { check_max( 2.h * task.attempt, 'time' ) }
    }

    withLabel:process_low {
        cpus   = { check_max( adaptive_cpus(2), 'cpus' ) }
        memory = { check_max( adaptive_memory(8.GB, 'low'), 'memory' ) }
        time   = { check_max( 4.h * task.attempt, 'time' ) }
    }

    withLabel:process_medium {
        cpus   = { check_max( adaptive_cpus(4), 'cpus' ) }
        memory = { check_max( adaptive_memory(16.GB, 'medium'), 'memory' ) }
        time   = { check_max( 6.h * task.attempt, 'time' ) }
    }

    withLabel:process_high {
        cpus   = { check_max( adaptive_cpus(8), 'cpus' ) }
        memory = { check_max( adaptive_memory(32.GB, 'high'), 'memory' ) }
        time   = { check_max( 12.h * task.attempt, 'time' ) }
    }

    withLabel:process_long {
        time   = { check_max( 24.h * task.attempt, 'time' ) }
    }

    withLabel:process_high_memory {
        memory = { check_max( adaptive_memory(64.GB, 'high_memory'), 'memory' ) }
        cpus   = { check_max( adaptive_cpus(4), 'cpus' ) }
    }

    // GPU process configuration with fallbacks
    withLabel:process_gpu {
        cpus   = { check_max( adaptive_cpus(8), 'cpus' ) }
        memory = { check_max( adaptive_gpu_memory(), 'memory' ) }
        time   = { check_max( 8.h * task.attempt, 'time' ) }

        accelerator = { 
            if (params.force_cpu) return null
            def requested_gpus = params.num_gpus ?: detect_gpu_count()
            def max_gpus = params.max_gpus_per_process ?: 1
            def gpu_count = Math.min(requested_gpus, max_gpus)
            return gpu_count > 0 ? [request: gpu_count, type: 'nvidia'] : null
        }

        containerOptions = { get_gpu_container_options() }

        beforeScript = '''
            # GPU validation and setup
            if command -v nvidia-smi >/dev/null 2>&1; then
                nvidia-smi || echo "Warning: nvidia-smi failed"
                export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
                export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
            else
                echo "Warning: nvidia-smi not found, GPU may not be available"
            fi
        '''
    }

    withLabel:process_gpu_high {
        cpus   = { check_max( adaptive_cpus(16), 'cpus' ) }
        memory = { check_max( adaptive_gpu_memory(true), 'memory' ) }
        time   = { check_max( 16.h * task.attempt, 'time' ) }

        accelerator = { 
            if (params.force_cpu) return null
            def requested_gpus = params.num_gpus ?: detect_gpu_count()
            def max_gpus = params.max_gpus_per_process ?: 4
            def gpu_count = Math.min(requested_gpus, max_gpus)
            return gpu_count > 0 ? [request: gpu_count, type: 'nvidia'] : null
        }

        containerOptions = { get_gpu_container_options() }
    }

    withLabel:error_ignore {
        errorStrategy = 'ignore'
    }

    withLabel:error_retry {
        errorStrategy = 'retry'
        maxRetries    = 2
    }

    // Process-specific configurations with adaptive resources
    withName:VALIDATE_INPUTS {
        cpus   = { check_max( 1, 'cpus' ) }
        memory = { check_max( adaptive_memory(4.GB, 'validation'), 'memory' ) }
        time   = { check_max( 1.h * task.attempt, 'time' ) }
    }

    withName:RESOLVI_PREPROCESS {
        cpus   = { check_max( adaptive_cpus(4), 'cpus' ) }
        memory = { check_max( adaptive_memory(16.GB, 'preprocess'), 'memory' ) }
        time   = { check_max( 4.h * task.attempt, 'time' ) }
    }

    withName:RESOLVI_TRAIN {
        cpus   = { check_max( adaptive_cpus(8), 'cpus' ) }
        memory = { check_max( adaptive_gpu_memory(true), 'memory' ) }
        time   = { check_max( 12.h * task.attempt, 'time' ) }
        label  = 'process_gpu_high'
    }

    withName:RESOLVI_ANALYZE {
        cpus   = { check_max( adaptive_cpus(4), 'cpus' ) }
        memory = { check_max( adaptive_memory(16.GB, 'analyze'), 'memory' ) }
        time   = { check_max( 6.h * task.attempt, 'time' ) }
    }

    withName:RESOLVI_VISUALIZE {
        cpus   = { check_max( adaptive_cpus(2), 'cpus' ) }
        memory = { check_max( adaptive_memory(8.GB, 'visualize'), 'memory' ) }
        time   = { check_max( 4.h * task.attempt, 'time' ) }
    }

    withName:SCVIVA_TRAIN {
        cpus   = { check_max( adaptive_cpus(8), 'cpus' ) }
        memory = { check_max( adaptive_gpu_memory(true), 'memory' ) }
        time   = { check_max( 12.h * task.attempt, 'time' ) }
        label  = 'process_gpu_high'
    }

    withName:SCVIVA_ANALYZE {
        cpus   = { check_max( adaptive_cpus(4), 'cpus' ) }
        memory = { check_max( adaptive_memory(16.GB, 'analyze'), 'memory' ) }
        time   = { check_max( 6.h * task.attempt, 'time' ) }
    }

    withName:get_software_versions {
        cache = false
        cpus   = { check_max( 1, 'cpus' ) }
        memory = { check_max( 2.GB, 'memory' ) }
        time   = { check_max( 30.min, 'time' ) }
    }
}

// Resource scaling functions
def adaptive_cpus(base_cpus) {
    def available = get_available_cpus()
    def scale_factor = get_scale_factor()
    def scaled = Math.ceil(base_cpus * scale_factor)
    return Math.min(scaled, available)
}

def adaptive_memory(base_memory, process_type) {
    def available = get_available_memory()
    def scale_factor = get_memory_scale_factor(process_type)
    def scaled = base_memory.toBytes() * scale_factor
    return Math.min(scaled, available * 0.8) as nextflow.util.MemoryUnit
}

def adaptive_gpu_memory(high_memory = false) {
    def base = high_memory ? 64.GB : 32.GB
    def gpu_count = detect_gpu_count()
    def gpu_memory_per_device = get_gpu_memory_per_device()

    if (gpu_count > 0 && gpu_memory_per_device > 0) {
        // Scale memory based on GPU memory availability
        def total_gpu_memory = gpu_count * gpu_memory_per_device
        def suggested_memory = Math.min(base.toBytes(), total_gpu_memory * 0.8)
        return suggested_memory as nextflow.util.MemoryUnit
    }

    return adaptive_memory(base, 'gpu')
}

def get_gpu_container_options() {
    def options = ''
    if (workflow.containerEngine == 'singularity') {
        options += ' --nv'
    } else if (workflow.containerEngine == 'docker') {
        options += ' --gpus all'
    }
    return options
}
